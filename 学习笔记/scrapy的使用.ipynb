{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy框架\n",
    "# scrapy框架的基本使用\n",
    "# 环境安装：\n",
    "# pip install wheel\n",
    "# 下载twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted\n",
    "# pip install twisted\n",
    "# pip install pywin32\n",
    "# pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy的基本使用\n",
    "# 创建一个工程：scrapy startproject xxxPro\n",
    "# 在spiders子目录中创建一个爬虫文件\n",
    "# scrapy genspider spiderName www.xxx.com\n",
    "# 执行工程\n",
    "# scrapy crawl spiderName\n",
    "# 执行\n",
    "# scrapy crawl spiderName\n",
    "# 不输出日志 scrapy crawl spiderName --nolog\n",
    "# 不输出日志也可以在setting文件中添加 LOG_LEVEL = 'ERROR'\n",
    "\n",
    "# settinga.py\n",
    "# Obey robots.txt rules\n",
    "# ROBOTSTXT_OBEY = False\n",
    "# 显示指定类型的日志信息\n",
    "# LOG_LEVEL = 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy 数据解析\n",
    "# 示例 爬取糗事百科https://www.qiushibaike.com/text/作者名称以及段子内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy持久化存储\n",
    "# 基于终端指令\n",
    "# 要求：只可以将parse方法的返回值存储到本地的文本文件中\n",
    "# scrapy crawl qiubai -o ./qiubai.csv 将结果输出到csv文件\n",
    "# 持久化存储对应的文件类型只可以为：json jsonlines jl csv xml marshal pickle\n",
    "# 指令：scrapy crawl xxx -o filePath\n",
    "# 好处：简洁高效便捷\n",
    "# 缺点：局限性比较强 数据只可以存储到指定后缀的文本文件中\n",
    "\n",
    "# 基于管道\n",
    "# 编码流程\n",
    "# 1.数据解析\n",
    "# 2.在item类中定义相关的属性\n",
    "# 3.将解析的数据封装存储到item类型的对象\n",
    "# 4.将item类型的对象提交给管道进行持久化存储的操作\n",
    "# 5.在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作\n",
    "# 6.在配置文件中开启管道\n",
    "\"\"\"\n",
    "ITEM_PIPELINES = {\n",
    "   'qiubaiPro.pipelines.QiubaiproPipeline': 300,\n",
    "}\n",
    "\"\"\"\n",
    "# 好处\n",
    "# 通用性强\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一份数据存到本地 一份数据存到数据库\n",
    "# 管道文件中一个管道类对应的是将数据存储到一种平台\n",
    "# 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于spider的全站数据爬取\n",
    "# 就是将网站中某板块下的全部页码对应的页面数据进行爬取\n",
    "# www.521609.com\n",
    "# 爬取该网站中所有照片的名称\n",
    "# 实现方式\n",
    "# 1.将所有页面的url添加到start_urls列表中（不推荐）\n",
    "# 2.自行手动进行请求发送（推荐）\n",
    "# yield scrapy.Request(url, callback) callback专门用于数据解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy五大核心组件\n",
    "\n",
    "# Spider\n",
    "# 爬虫是主要干活的 用于从特定的网页中提取自己需要的信息 即所谓的实体item 用户也可以从中提取出链接让spider继续抓取下一个页面\n",
    "\n",
    "# 引擎\n",
    "# 用来进行整个系统的数据流处理 触发事务（框架核心）\n",
    "\n",
    "# 调度器\n",
    "    # 过滤器\n",
    "    # 队列\n",
    "# 用来接收引擎发送过来的请求 压入队列中 并在引擎再次请求的时候返回 可以想象成一个URL的优先队列 由它决定下一个抓取的网址是什么 同时去除重复的网址\n",
    "\n",
    "# 管道\n",
    "# 负责处理爬虫从网页中抽取的实体 主要的功能是持久化实体 验证实体的有效性 清除不需要的信息 当页面被爬虫解析后 将被发送到管道 并经过几个特定次序处理数据\n",
    "\n",
    "# 下载器\n",
    "# 用于下载网页内容 并将网页内容返回给spider scrapy下载器是建立在twisted这个高效的异步模型上的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求传参\n",
    "# 使用场景：如果爬取解析的数据不在同一张页面中（深度爬取）\n",
    "# 需求：爬取boss的岗位名称以及详情页的岗位描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片数据爬取之ImagePipeline\n",
    "# 基于scrapy爬取字符串类型的数据 和 爬取图片类型的数据的区别\n",
    "# 字符串：只需要基于xpath进行解析且提交管道进行持久化存储\n",
    "# 图片：xpath解析出的图片src的属性值 单独对图片地址发起请求获取图片二进制类型的数据\n",
    "\n",
    "# ImagePipeline\n",
    "# 只需要将img的src属性值进行解析 提交到管道 管道就会对图片的src进行请求发送 获取图片的二进制数据 且还会帮我们进行二进制存储\n",
    "\n",
    "# 需求：爬取站长素材中的高清图片\n",
    "# 使用流程\n",
    "# 数据解析（图片的地址）\n",
    "# 将存储图片地址的item提交到指定的管道类\n",
    "# 再管道文件中自己定制一个基于ImagesPipeLine的一个管道类\n",
    "# 重写以下三个方法\n",
    "# get_media_request\n",
    "# file_path\n",
    "# item_completed\n",
    "# 在配置文件中：\n",
    "# 指定图片的存储目录 IMAGES_STORE = ''\n",
    "# 指定开启的管道：自己定制的管道类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中间件\n",
    "# 下载中间件\n",
    "# 位置：引擎和下载器之间\n",
    "# 作用：批量拦截到整个工程中发起的所有请求和响应\n",
    "# 拦截请求：\n",
    "# UA伪装\n",
    "# 代理IP设定\n",
    "# 拦截响应：\n",
    "# 篡改响应数据，响应对象\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
